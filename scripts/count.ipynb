{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "from jsonpath_ng.ext import parse\n",
    "import pandas\n",
    "\n",
    "ICD10_CODE = parse('$..CodeList[?(@.Kat==\"ICD10\")].Code')\n",
    "\n",
    "def file_to_codes(json_file):\n",
    "    with open(json_file) as jf:\n",
    "        data = json.load(jf)\n",
    "        return [ match.value for match in ICD10_CODE.find(data) ]\n",
    "\n",
    "def excel_to_codes(excel_file='codes/Code_Lists_ETEV_Hemo_ETEA_20200228.xlsx'):    \n",
    "    icd10codes = []\n",
    "    icd10_2_codes = []\n",
    "    sheets = pandas.read_excel(excel_file, sheet_name=None)\n",
    "\n",
    "    for name, sheet in sheets.items():\n",
    "        try:\n",
    "            codes = sheet['ICD10GM_code'].tolist()\n",
    "            codes.extend(sheet['ICD10GM2_code2'].tolist())\n",
    "            codes = set(codes)\n",
    "            pandas.DataFrame(codes).to_csv('codes/' + name + '.txt',index=False,header=False)\n",
    "        except KeyError:\n",
    "            i=0\n",
    "\n",
    "def count(json_directory='data_json'):\n",
    "    total_diagnoses_counter = 0\n",
    "    matching_diagnoses_counter = 0\n",
    "    diagnoses_counter = {}\n",
    "    \n",
    "    for report in os.listdir(json_directory):\n",
    "        if not report.endswith(\".json\"): continue;\n",
    "        print('processing report ', report)\n",
    "\n",
    "        codes = file_to_code(os.path.join(json_directory, report))\n",
    "\n",
    "        total_diagnoses_counter += len(codes)\n",
    "        matching_diagnoses = set(codes) & excel_to_codes()\n",
    "        matching_diagnoses_counter = len(matching_diagnoses)\n",
    "        for match in matching_diagnoses:\n",
    "            if match in diagnoses_counter:\n",
    "                diagnoses_counter[match] += 1\n",
    "            else:\n",
    "                diagnoses_counter[match] = 1\n",
    "\n",
    "    w = csv.writer(open(\"output.csv\", \"w\"))\n",
    "    w.writerow(['total diagnoses', total_diagnoses_counter])\n",
    "    w.writerow(['matching diagnoses', matching_diagnoses_counter])\n",
    "    for key, val in diagnoses_counter.items():\n",
    "        w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_subset(counts='count.csv',size=300):\n",
    "    import random\n",
    "\n",
    "    diagnoses = codes_to_documents()\n",
    "    \n",
    "    counts = pandas.read_csv('output.csv', index_col=0, squeeze=True).to_dict()\n",
    "    counts.pop('total diagnoses')\n",
    "    total = counts.pop('matching diagnoses')\n",
    "    factor = size/int(total)\n",
    "    \n",
    "    subset = []\n",
    "    \n",
    "    for diagnosis, count in counts.items():\n",
    "        number = int(count * factor) + 1\n",
    "        \n",
    "        if number > len(diagnoses[diagnosis]):\n",
    "            number = len(diagnoses[diagnosis])\n",
    "        \n",
    "        subset.extend(random.sample(diagnoses[diagnosis], number))\n",
    "        \n",
    "    subset = set(subset)\n",
    "    with open('subset.txt','w') as outfile:\n",
    "        outfile.write(\"\\n\".join(subset))\n",
    "    print(subset)\n",
    "\n",
    "    \n",
    "    \n",
    "def codes_to_documents(inpath='test_data'):\n",
    "    all_codes = excel_to_codes()\n",
    "    diagnoses = {}\n",
    "    \n",
    "    for report in os.listdir(inpath):\n",
    "        if not report.endswith(\".json\"): continue;\n",
    "        print('processing report ', report)\n",
    "\n",
    "        codes = file_to_code(os.path.join(inpath, report))\n",
    "\n",
    "        matching_diagnoses = set(codes) & all_codes\n",
    "        matching_diagnoses_counter = len(matching_diagnoses)\n",
    "        for match in matching_diagnoses:\n",
    "            if match in diagnoses:\n",
    "                diagnoses[match].append(report)\n",
    "            else:\n",
    "                diagnoses[match] = [report]\n",
    "        \n",
    "    return diagnoses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codes(infile):\n",
    "    codes = open(infile).read().splitlines()\n",
    "    codes = [ code.split('!')[0] for code in codes ]\n",
    "    codes_expand = [ code.split('_')[0] for code in codes if '_' in code ]\n",
    "    codes = [ code for code in codes if not '_' in code ]\n",
    "    \n",
    "    return codes, codes_expand\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n",
      "het patiert\n"
     ]
    }
   ],
   "source": [
    "def predict_ade(codes,haystack):\n",
    "    classification = []\n",
    "    \n",
    "    vte = any(code in haystack['vte_codes'] for code in codes)\n",
    "    vte = vte or any(code.startswith(tuple(haystack['vte_codes_expand'])) for code in codes)\n",
    "    \n",
    "    if vte:\n",
    "        \n",
    "        pe = any(code in haystack['pe_codes'] for code in codes)\n",
    "        pe = pe or any(code.startswith(tuple(haystack['pe_codes_expand'])) for code in codes)\n",
    "        if pe:\n",
    "            classification.append('pe')\n",
    "\n",
    "        stroke = any(code in haystack['stroke_codes'] for code in codes)\n",
    "        stroke = stroke or any(code.startswith(tuple(haystack['stroke_codes_expand'])) for code in codes)\n",
    "        if stroke:\n",
    "            classification.append('stroke')\n",
    "\n",
    "        ami = any(code in haystack['ami_codes'] for code in codes)\n",
    "        ami = ami or any(code.startswith(tuple(haystack['ami_codes_expand'])) for code in codes)\n",
    "        if ami:\n",
    "            classification.append('ami')\n",
    "    \n",
    "    ade_hemo = any(code in haystack['ade_hemo_codes'] for code in codes)\n",
    "# not necessary, there are not _ in the ade_hemo codes\n",
    "# ade_hemo = ade_hemo or any(code.startswith(tuple(ade_hemo_codes_expand)) for code in codes)\n",
    "    \n",
    "    if ade_hemo:\n",
    "        \n",
    "        sev_hemo = any(code in haystack['sev_hemo_codes'] for code in codes)\n",
    "        sev_hemo = sev_hemo or any(code.startswith(tuple(haystack['sev_hemo_codes_expand'])) for code in codes)\n",
    "        if sev_hemo:\n",
    "            classification.append('sev_hemo')\n",
    "        \n",
    "        \n",
    "        \n",
    "    if len(classification) < 1:\n",
    "        categorisation = ['unclassified']\n",
    "        \n",
    "    return classification\n",
    "    \n",
    "    # if doesn't yield enough, try adding Hemo & ( transfu | death )\n",
    "    \n",
    "def predict_directory(inpath='subset_300/*json'):\n",
    "    \n",
    "    haystack = { 'pe_codes':[], \n",
    "                 'pe_codes_expand':[],\n",
    "                 'vte_codes':[],\n",
    "                 'vte_codes_expand':[],\n",
    "                 'stroke_codes':[],\n",
    "                 'stroke_codes_expand':[],\n",
    "                 'ami_codes':[],\n",
    "                 'ami_codes_expand':[],\n",
    "                 'ade_hemo_codes':[],\n",
    "                 'ade_hemo_codes_expand':[],\n",
    "                 'sev_hemo_codes':[],\n",
    "                 'sev_hemo_codes_expand':[]\n",
    "               }\n",
    "    \n",
    "    haystack['pe_codes'], haystack['pe_codes_expand'] = get_codes('codes/PE.txt')\n",
    "    haystack['vte_codes'], haystack['vte_codes_expand'] = get_codes('codes/ADE_VTE.txt')\n",
    "    haystack['stroke_codes'], haystack['stroke_codes_expand'] = get_codes('codes/Stroke.txt')\n",
    "    haystack['ami_codes'], haystack['ami_codes_expand'] = get_codes('codes/AMI.txt')\n",
    "    haystack['ade_hemo_codes'], haystack['ade_hemo_codes_expand'] = get_codes('codes/ADE_Hemo.txt')\n",
    "    haystack['sev_hemo_codes'], haystack['sev_hemo_codes_expand'] = get_codes('codes/Sev_Hemo.txt')\n",
    "    \n",
    "    classified = { 'stroke' : [],\n",
    "                   'sev_hemo' : [],\n",
    "                   'ami' : [],\n",
    "                   'pe' : []}\n",
    "    \n",
    "    import glob\n",
    "    import os.path\n",
    "    for report in glob.glob(\"subset_300/*.json\"):\n",
    "        codes = file_to_codes(report)\n",
    "        \n",
    "        classifications = predict_ade(codes,haystack)\n",
    "        if classifications != []:\n",
    "            \n",
    "            for classification in classifications:\n",
    "                classified[classification].append(os.path.split(report)[1])\n",
    "                print('het patiert')\n",
    "                \n",
    "    for ade, documents in classified.items():\n",
    "        with open(ade + '.txt','w') as f:\n",
    "            f.write(\"\\n\".join(documents))\n",
    "                    \n",
    "\n",
    "predict_directory(inpath='subset_300/*json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
